---
title: "Statewide Survey Non-Response Analysis"
author: "Eve Perry"
date: today
format: 
  bhhi-quarto-html:
    code-fold: true
---

```{r setup}
library(bhhi.doc)
library(tidyverse)
library(janitor)
library(kableExtra)
library(ggraph)
library(tidygraph)
library(tidymodels)
library(doParallel)
library(ggridges)
library(stacks)
library(finetune)
library(gt)
library(glue)

set.seed(510)

if (Sys.info()["sysname"] == "Linux") {
  cores = ifelse(Sys.getenv("NSLOTS") == "", 3, Sys.getenv("NSLOTS"))
  cl <- makeForkCluster(cores)
  registerDoParallel(cl)
} else {
  plan(multisession, workers = 3)
}

tuning_v_folds = 10
tuning_repeats = 5
tuning_grid_size = 25
```

# Defining responses and non-responses

To determine the non-response rate, we must first define responses. We only consider those who finished the survey (`survey_count == 1`) as responses. Those who declined to talk to the interviewer, whose eligibility we could not determine (for language or other reasons), who did not consent to the full interview, or did not finish the survey are considered non-responses. We exclude those determined to be ineligible from the non-responses since they are, by definition, not in the survey frame.

Below is a chart that shows the participants' progression through the interview process:

```{r}
#| echo: true
survey_path = ifelse(
  Sys.info()["sysname"] == "Linux",
  # wynton
  "/wynton/protected/home/kushel/eveperry/statewide-survey/processed-survey-data/statewide_survey_processed.rds",
  # everywhere else
  "Y:/Research/BHHI/statewide_survey_processed_data/latest/statewide_survey_processed.rds"
)

raw_surveys = read_rds(survey_path)

prepare_survey_data = function(data) {
  data %>% 
    filter(is.na(rds) | rds == 0) %>% 
    mutate(
      county = fct_relevel(
        as_factor(county),
        c(
          "Sonoma",
          "Placer",
          "Santa Clara",
          "Fresno/Madera",
          "Butte",
          "Los Angeles",
          "San Diego",
          "Sacramento"
        )
      ),
      language_issues = eligible_yn == -4,
      declined = eligible_yn == -3,
      could_not_determine_eligibility = eligible_yn == -2,
      eligibility_determined = eligible_yn >= -1,
      ineligible = eligible_yn %in% -1:0,
      eligible = eligible_yn %in% 1:3,
      consented_at_start = eligible_yn %in% 1:2,
      did_not_consent = eligible_yn == 3,
      did_not_finish = consented_at_start & survey_count == 0,
      response = survey_count == 1,
      non_response = !(ineligible | survey_count == 1),
      ## perceived demographics
      # age
      perceived_age = fct_explicit_na(as_factor(perceived_age), "Missing"),
      # disability
      perceived_disability = fct_explicit_na(
        as_factor(perceived_disability), "Missing"
      ),
      perceived_disability = fct_recode(
        perceived_disability,
        "Disabled" = "Yes",
        "Not Disabled" = "No"
      ),
      # intoxication
      perceived_intoxication = fct_explicit_na(
        as_factor(perceived_intox), "Missing"
      ),
      perceived_intoxication = fct_recode(
        perceived_intoxication,
        "Intoxicated" = "Yes",
        "Not Intoxicated" = "No"
      ),
      # gender
      perceived_gender = fct_explicit_na(
        as_factor(perceived_gender), "Missing"
      ),
      perceived_gender = fct_recode(
        perceived_gender,
        "Transgender" = "Transgender, Genderqueer, Gender non-binary or Unclear"
      ),
      # race
      perceived_race = fct_explicit_na(
        as_factor(perceived_race_onecat), "Missing"
      ),
      perceived_race = fct_recode(
        perceived_race,
        "Asian/Pacific Islander" = "Asian",
        "Asian/Pacific Islander" = "Pacific Islander, Samoan, Hawaiian",
        "Black" = "Black, African-American, African",
        "White" = "White, Caucasian, European American",
        "Latinx" = "Latinx, Hispanic, Latin American",
        "Native American" = "Native American, Alaska Native",
        "Multiracial" = "Mixed/Multiracial",
        "Other" = "Different race or ethnic ID, or unclear",
        "Missing" = "FIX CODING ERROR"
      ),
      across(
        c(perceived_age, perceived_disability, perceived_gender, perceived_intoxication, perceived_race), fct_relevel, sort),
      across(
        c(perceived_age, perceived_disability, perceived_gender, perceived_intoxication, perceived_race), fct_relevel, "Missing", after = Inf)
    )
}

surveys = prepare_survey_data(raw_surveys)
```

```{r}
#| fig-height: 8.1
nodes = surveys %>% 
  summarise(
    approached = n(),
    declined_eligibility_determination = sum(declined),
    eligibility_determined = sum(eligibility_determined),
    no_eligibility_determination = sum(
      language_issues, could_not_determine_eligibility
    ),
    eligible = sum(did_not_consent, did_not_finish, response),
    ineligible = sum(ineligible),
    consented = sum(did_not_finish, response),
    did_not_consent = sum(did_not_consent),
    finished = sum(response),
    did_not_finish = sum(did_not_finish)
  ) %>% 
  pivot_longer(everything(), names_to = "node", values_to = "count") %>% 
  mutate(
    label = paste(
      str_to_title(str_replace_all(node, "_", " ")),
      count,
      sep = "\n"
    ),
    response = case_when(
      node == "finished" ~ "Response",
      node == "ineligible" ~ "Excluded",
      node %in% c(
        "approached", "eligibility_determined", "eligible", "consented"
      ) ~ "Intermediate",
      TRUE ~ "Non-Response"
    )
  )

edges = tribble(
  ~ from, ~ to,
  "approached", "declined_eligibility_determination",
  "approached", "eligibility_determined",
  "approached", "no_eligibility_determination",
  "eligibility_determined", "ineligible",
  "eligibility_determined", "eligible",
  "eligible", "did_not_consent",
  "eligible", "consented",
  "consented", "did_not_finish",
  "consented", "finished"
)

tbl_graph(nodes, edges) %>% 
  ggraph(layout = "tree") +
  geom_edge_diagonal() +
  geom_node_label(
    aes(label = label, color = response),
    key_glyph = draw_key_point
  ) +
  scale_color_manual(
    name = NULL,
    values = c(
      "Response" = bhhi_palette("green"),
      "Non-Response" = bhhi_palette("red"),
      "Excluded" = bhhi_palette("grey"),
      "Intermediate" = "black"
    ),
    breaks = c("Response", "Non-Response", "Excluded")
  ) +
  coord_cartesian(xlim = c(-1.6, 1.4)) +
  theme(legend.position = c(0.9, 0.25)) +
  labs(caption = "Note: All counts are unweighted.")
```

# Non-response rate

```{r}
weighted_sum = function(var, weight) {
  sum(var * weight, na.rm = TRUE)
}
```


## Initial calculation

The initial calculation of the non-response rate simply divides the non-responses by the sum of response and non-responses:

$$ Non\ Response\ Rate = \frac{Non\ Response}{Response + Non\ Response} $$
And shown by county below:

```{r}
#| echo: true
initial_non_response = surveys %>% 
  group_by(county) %>% 
  summarise(
    approached = n(),
    across(c(response, non_response), weighted_sum, initial_weight)
  ) %>%
  adorn_totals() %>% 
  mutate(
    initial_non_response = non_response / (response + non_response)
  )
```

```{r}
weight_note_text = 
  "Weighted with inverse venue & individual selection probabilities."
weight_note = function(kbl) {
  kbl %>% 
    add_footnote(
      paste0("<div class=kbl-fn>", weight_note_text,"</div>"),
      notation = "none",
      escape = FALSE
    )
}
        
initial_non_response %>% 
  relocate(non_response, .after = approached) %>% 
  kbl(
    col.names = c(
      "County", "Approached", "Non-Response",
      "Response", "Initial Non-Response Rate"
    ),
    digits = c(rep(0, 4), 3)
  ) %>% 
  kable_styling() %>% 
  weight_note()
```

## Adjusting the non-responses for eligibility

In the eligibility screening, we've found that a small number of respondents were ineligible. We want to adjust our non-response count for respondents who declined to participate in the eligibility screen and respondents for whom we could not determine eligibility. 

Assuming that respondents for whom we could determine eligibility are not different than those for whom we could not determine eligibility (see [figures](#perceived-demographics-by-eligibility-determination-status) below), some of the respondents without eligibility determination would have been determined to be ineligible. We also want to exclude these people from our non-response rate calculation.

To get the number of  respondents without eligibility determination who would have been determined to be ineligible, we multiply the share of ineligible respondents among respondents with eligibility determinations by the number of of respondents without eligibility. We then subtract that from the non-response count to get the adjusted non-response count:

$$ Adjusted\ Non\ Response = Non\ Response - Eligiblity\ Not\ Determined * \frac{Ineligible}{Eligibility\ Determined} $$

Ultimately, the adjustment does not have a large effect as the adjustment is small in every county:

```{r}
#| echo: true
adjusted_non_response = surveys %>% 
  group_by(county) %>% 
  summarise(
    approached = sum(initial_weight, na.rm = TRUE),
    across(c(non_response, eligibility_determined, ineligible), weighted_sum, initial_weight)
  ) %>%
  mutate(eligibility_not_determined = approached - eligibility_determined) %>% 
  adorn_totals() %>% 
  mutate(
    adjusted_non_response = 
      non_response - eligibility_not_determined * ineligible / eligibility_determined
  )
```

```{r}
adjusted_non_response %>% 
  select(
    county, eligibility_not_determined, ineligible, 
    eligibility_determined, non_response, adjusted_non_response
  ) %>% 
  kbl(
    col.names = c(
      "County", "Eligibility Not Determined", "Ineligible",
      "Eligibility Determined", "Non-Response", "Adjusted Non-Response"
    ),
    digits = c(rep(0, 5), 1)
  ) %>% 
  kable_styling %>% 
  weight_note()
```

### Perceived demographics by eligibility determination status
Respondents' perceived demographics are similar for those with and without eligibility determinations. This provides assurance that the ineligibility rate among those with determinations can be used to impute the number of likely ineligibility for those without determinations. 

```{r}
eligibility_determination = surveys %>% 
  mutate(
    eligibility_determined = factor(
      eligibility_determined,
      levels = c(TRUE, FALSE),
      labels = c("Eligibility Determined", "Eligibility Not Determined")
    )
  )

demographic_categories = c(
  "age", "disability", "gender", "intoxication", "race"
)

eligibility_determination_plot_data = map(
  demographic_categories,
  function(var_stem) {
    var = paste0("perceived_", var_stem)
    eligibility_determination %>% 
      group_by(eligibility_determined, .data[[var]]) %>% 
      summarise(n = sum(initial_weight, na.rm = TRUE)) %>% 
      mutate(
        category = str_to_sentence(str_replace_all(var, "_", " ")),
        share = n / sum(n)
      ) %>% 
      rename(subcategory = all_of(var)) %>% 
      ungroup() %>% 
      complete(
        category, subcategory, eligibility_determined,
        fill = list(n = 0, share = 0)
      )
  }
)

eligibility_determination_plot_data %>% 
  list_rbind() %>%
  ggplot(
    aes(
      x = share,
      y = fct_relevel(fct_rev(subcategory), "Missing", after = 0),
      fill = eligibility_determined,
      color = eligibility_determined
    )
  ) +
  geom_point(pch = 21, size = 3, alpha = 0.75) +
  scale_x_continuous(name = NULL, labels = scales::percent, limits = 0:1) +
  labs(
    title = "Demographic distribution by eligibility determination status",
    y = NULL,
    caption = weight_note_text
  ) +
  guides(
    fill = guide_legend(title = NULL),
    color = guide_legend(title = NULL)
  ) +
  facet_wrap(vars(category), scales = "free_y") +
  theme(legend.position = c(0.85, 0.25))
```

## Adjusted non-response rate

The adjusted non-response rate is then the adjusted non-response count divided by the sum of the adjusted non-response count and the responses:

$$ Adjusted\ Non\ Response\ Rate = \frac{Adjusted\ Non\ Response}{Response + Adjusted\ Non\ Response} $$

The adjusted non-response rate is slightly lower than the unadjusted rate:

```{r}
#| echo: true
non_response_rate = surveys %>% 
  group_by(county) %>% 
  summarise(
    approached = sum(initial_weight, na.rm = TRUE),
    across(
      c(response, non_response, ineligible, eligibility_determined),
      weighted_sum, initial_weight
    )
  ) %>% 
  mutate(eligibility_not_determined = approached - eligibility_determined) %>% 
  adorn_totals() %>% 
  mutate(
    adjusted_non_response = 
      non_response - eligibility_not_determined * ineligible / eligibility_determined,
    unadjusted_non_response_rate = 
      non_response / (response + non_response),
    adjusted_non_response_rate = 
      adjusted_non_response / (response + adjusted_non_response)
  ) 
```

```{r}
non_response_rate %>% 
  select(
    county, response, adjusted_non_response,
    adjusted_non_response_rate, non_response, unadjusted_non_response_rate
  ) %>% 
  kbl(
    col.names = c(
      "County", "Response", "Non-Response", "Rate", "Non-Response", "Rate"
    ),
    digits = c(0, 0, 1, 3, 0, 3)) %>% 
  add_header_above(c(" " = 2, "Adjusted" = 2, "Unadjusted" = 2)) %>% 
  kable_styling() %>% 
  weight_note()
```

The non-response rate substantially declines with the order in which we visited the counties.

```{r}
non_response_rate %>% 
  ungroup() %>% 
  filter(county != "Total") %>% 
  mutate(county = factor(county, levels(surveys$county), ordered = TRUE)) %>% 
  arrange(county) %>% 
  ggplot(aes(x = county, y = adjusted_non_response_rate)) + 
  geom_point() +
  geom_smooth(aes(x = 1:8), method = "lm", se = FALSE) +
  scale_y_continuous(labels = scales::percent, limits = c(0, 0.35)) +
  labs(
    title = "Non-response rate by county",
    subtitle = "Counties in order of visit",
    x = NULL,
    y = NULL
  )
```


# Non-response by perceived demographics

## Perceived demographic missingness
```{r}
#| echo: true
demo_missingness = surveys %>%
  group_by(county) %>% 
  summarise(
    approached = n(),
    one_plus_missing = sum(
      perceived_age == "Missing" |
      perceived_disability == "Missing" |
      perceived_intoxication == "Missing" |
      perceived_gender == "Missing" |
      perceived_race == "Missing"
    ),
    across(
      c(
        perceived_age, perceived_disability,
        perceived_intoxication, perceived_gender, perceived_race
      ),
      ~ sum(.x == "Missing")
    )
  ) %>% 
  adorn_totals() %>% 
  mutate(
    across(
      c(
        one_plus_missing, perceived_age, perceived_disability,
        perceived_intoxication, perceived_gender,  perceived_race
      ),
      ~ .x / approached
    )
  )
```

Overall, the missingness in perceived demographics is low, at around `r scales::percent(demo_missingness$one_plus_missing[demo_missingness$county == "Total"])`. Most counties show similarly low levels of missingness, with Placer as a notable exception (although still relatively low, at around `r scales::percent(demo_missingness$one_plus_missing[demo_missingness$county == "Placer"])`).

```{r}
demo_missingness %>% 
  select(-approached) %>% 
  kbl(
    col.names = c(
      "County", "1+ Missing", "Age", "Disability",
      "Intoxication", "Gender", "Race"
    ),
    digits = 3
  ) %>% 
  kable_styling() %>% 
  add_footnote(
    "<div class=kbl-fn>Table is unweighted.</div>",
    notation = "none",
    escape = FALSE
  )
```

```{r}
outcome_by_demo = function(demo) {
  surveys %>%
  group_by({{ demo }}) %>% 
  summarise(
    n = n(),
    approached = sum(initial_weight, na.rm = TRUE),
    eligibility_determined = weighted_sum(
      eligibility_determined,
      initial_weight
    ),
    ineligible = weighted_sum(ineligible, initial_weight),
    non_response = weighted_sum(non_response, initial_weight),
    response = weighted_sum(response, initial_weight)
  ) %>% 
  adorn_totals() %>% 
  mutate(
    ineligible = ineligible / eligibility_determined,
    eligibility_determined = eligibility_determined / approached,
    non_response = non_response / (non_response + response)
  ) %>% 
  select(
    {{ demo }}, n, eligibility_determined,
    ineligible, non_response
  )
}

outcome_by_demo_table = function(outcomes, demo_label) {
  outcomes %>% 
    kbl(
      col.names = c(
        demo_label, "N", "Eligibility Determined",
        "Ineligible", "Non-Response"
      ),
      digits = c(rep(0, 2), rep(3, 3))
    ) %>% 
    kable_styling() %>% 
    weight_note()
}
```

## Perceived age
```{r}
outcome_by_demo(perceived_age) %>% 
  outcome_by_demo_table("Age")
```

## Perceived disability
```{r}
outcome_by_demo(perceived_disability) %>% 
  outcome_by_demo_table("Disability")
```

## Perceived gender
```{r}
outcome_by_demo(perceived_gender) %>% 
  outcome_by_demo_table("Gender")
```

## Perceived intoxication
```{r}
outcome_by_demo(perceived_intoxication) %>% 
  outcome_by_demo_table("Intoxication")
```

## Perceived race
```{r}
outcome_by_demo(perceived_race) %>% 
  outcome_by_demo_table("Race")
```

## Site category
```{r}
outcome_by_demo(site_category) %>% 
  outcome_by_demo_table("Site category")
```

# Perceived demographics reliability

```{r}
demo_reliability_data = surveys %>% 
  filter(response) %>% 
  mutate(
    actual_age = cut(
      age_self_report,
      c(17, 24, 54, Inf),
      c("18-24  years old", "25-54 years old", "55+ years old")
    ),
    actual_age = fct_explicit_na(actual_age, "Missing"),
    age_match = actual_age == perceived_age,
    actual_disability = fct_recode(
      fct_explicit_na(as_factor(disability_2), "Missing"),
      "Disabled" = "Yes",
      "Not Disabled" = "No",
      "Missing" = "Refused",
      "Missing" = "Dont know"
    ),
    disability_match = actual_disability == perceived_disability,
    actual_gender = fct_recode(
      fct_explicit_na(as_factor(gender_3cat), "Missing"),
      "Male" = "Male [cisgender]",
      "Female" = "Female [cisgender]",
      "Transgender" = "Other [transgender, genderqueer, etc]"
    ),
    gender_match = actual_gender == perceived_gender,
    actual_race = fct_recode(
      fct_explicit_na(as_factor(race_7cat), "Missing"),
      "Asian/Pacific Islander" = "NH AAPI",
      "Black" = "NH Black",
      "White" = "NH White",
      "Latinx" = "Latinx/Hispanic",
      "Native American" = "NH Native American/Alaskan",
      "Multiracial" = "NH Multiracial",
      "Other" = "NH Other"
    ),
    race_match = actual_race == perceived_race,
  )

demo_reliability_data %>% 
  summarise(
    age_n = sum(perceived_age != "Missing" & actual_age != "Missing"),
    age_match = sum(
      age_match[perceived_age != "Missing" & actual_age != "Missing"]
    ),
    disability_n = sum(
      perceived_disability != "Missing" & actual_disability!= "Missing"
    ),
    disability_match = sum(
      disability_match[
        perceived_disability != "Missing" & actual_disability != "Missing"
      ]
    ),
    gender_n = sum(perceived_gender != "Missing" & actual_gender != "Missing"),
    gender_match = sum(
      gender_match[perceived_gender != "Missing" & actual_gender != "Missing"]
    ),
    race_n = sum(perceived_race != "Missing" & actual_race != "Missing"),
    race_match = sum(
      race_match[perceived_race != "Missing" & actual_race != "Missing"]
    )
  ) %>% 
  pivot_longer(
    cols = everything(),
    names_to = c("demo", "stat"), names_sep = "_",
    values_to = "value"
  ) %>% 
  pivot_wider(names_from = stat, names_sort = TRUE, values_from = value) %>% 
  mutate(match_rate = match / n) %>% 
  ggplot(aes(x = match_rate, y = fct_rev(str_to_sentence(demo)))) +
  geom_point(size = 12, color = bhhi_accent(), fill = bhhi_accent(), pch = 21) +
  geom_text(aes(label = scales::percent(match_rate, 1)), color = "grey95") +
  scale_x_continuous(name = NULL, limits = 0:1, labels = scales::percent) +
  labs(
    title = "Share of respondents with same actual and perceived demographic",
    y = NULL,
    caption = paste(
      "Results are unweighted.",
      "Respondents missing either an actual or perceived demographic are excluded for that demographic.", 
      sep = "\n"
    )
  )
```

# Modeling non-response

## Adjusted non-response weights

We adjust the weights of the respondents without an eligibility determination such that the weighted average non-resonse rate equals the adjusted non-response weight above. The weight adjustment is the unweighted rate of eligibility for the survey:

$$ Adjusted\ nonreponse\ weights_{eligibility\ not\ determined} = initial\_weight * \frac{\sum{eligible}}{\sum{eligibility\ determined}}$$

Then, we use these weights to predict non-response. The predictors in the model are county and the perceived demographics (age, disability, gender, intoxication, and race).

```{r}
#| echo: true
prepare_model_data = function(data) {
  data %>% 
    mutate(
      anrr_weight = if_else(
        eligibility_determined,
        initial_weight,
        initial_weight * (sum(eligible) / sum(eligibility_determined))
      ),
      anrr_weight = importance_weights(anrr_weight)
    ) %>% 
    filter(!ineligible) %>% 
    select(
      anrr_weight, non_response, perceived_age, perceived_disability,
      perceived_gender, perceived_intoxication, perceived_race, county,
      site_category, record_id
    ) %>% 
    mutate(
      across(where(is.factor), fct_drop),
      across(starts_with("perceived"), fct_recode, NULL = "Missing"),
      site_category = fct_recode(
        site_category,
        "Encampment" = "Hotspot",
        NULL = ""
      ),
      non_response = factor(
        non_response,
        levels = c(FALSE, TRUE),
        labels = c("Response", "Non-Response")
      )
    ) %>% 
    filter(!is.na(anrr_weight))
}

anrr_weights = prepare_model_data(surveys) 
```

## Candidate models

To predict non-response, we evaluate a set of individual models and an ensemble model that combines the individual models.

The only pre-processing step is to turn categorical variables into sets of dummy variables.

```{r}
#| echo: true

reference_levels = c(
  perceived_age = "18-24  years old",
  perceived_disability = "Not Disabled",
  perceived_gender = "Male",
  perceived_intoxication = "Not Intoxicated",
  perceived_race = "White",
  county = "Los Angeles",
  site_category = "Emergency Shelter"
)

anrr_recipe = anrr_weights %>%
  recipe(non_response ~ .) %>%
  update_role(record_id, new_role = "id variable") %>% 
  step_unknown(all_factor_predictors(), new_level = "Missing") %>%
  step_relevel(
    perceived_age,
    ref_level = reference_levels["perceived_age"]
  ) %>%
  step_relevel(
    perceived_disability,
    ref_level = reference_levels["perceived_disability"]
  ) %>%
  step_relevel(
    perceived_gender,
    ref_level = reference_levels["perceived_gender"]
  ) %>%
  step_relevel(
    perceived_intoxication,
    ref_level = reference_levels["perceived_intoxication"]
  ) %>%
  step_relevel(
    perceived_race,
    ref_level = reference_levels["perceived_race"]
  ) %>%
  step_relevel(
    county,
    ref_level = reference_levels["county"]
  ) %>%
  step_relevel(
    site_category,
    ref_level = reference_levels["site_category"]
  ) %>%
  step_dummy(all_predictors())
```


To allow for proper evaluation of our predictions, we first split the data into training (75% of the data) and test (25% of the data) sets.

```{r}
#| echo: true

anrr_split = initial_split(anrr_weights, strata = non_response)
```

To select the parameters for the models, we evaluate a random grid with `r tuning_grid_size` combinations of parameters with `r tuning_v_folds`-fold cross validation repeated `r tuning_repeats` times.

```{r}
#| echo: true

anrr_folds = vfold_cv(
  training(anrr_split), v = tuning_v_folds, repeats = tuning_repeats
)
```

### Individual models

The 5 individual models are: logistic regression (base R), logistic regression with lasso (`glmnet` package), random forest (`ranger` package), neural net (`nnet` packages), and gradient boosted trees (`xgboost` package). We also evaluate a stacked ensemble of 5 models.

```{r}
#| echo: true

logit_glm_model = logistic_reg() %>%
  set_mode("classification") %>% 
  set_engine("glm")

logit_glmnet_model = logistic_reg(penalty = tune(), mixture = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet")

rf_model = rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_mode("classification") %>% 
  set_engine("ranger")

nn_model = mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("nnet")

xgboost_model = boost_tree(
  trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), 
  loss_reduction = tune(), sample_size = tune()
) %>% 
  set_mode("classification") %>% 
  set_engine("xgboost")
```

```{r}
#| echo: true

weight_models = workflow_set(
  preproc = list(basic = anrr_recipe),
  models = list(
    logit_glm = logit_glm_model,
    logit_glmnet = logit_glmnet_model,
    random_forest = rf_model,
    xgboost = xgboost_model
  ),
  case_weights = anrr_weight
)

no_weight_models = workflow_set(
  preproc = list(basic = anrr_recipe),
  models = list(
    neural_net = nn_model
  )
)

tune_metrics = metric_set(
  roc_auc, accuracy, mn_log_loss, pr_auc, average_precision, f_meas
)

race_ctrl = control_race(
  save_pred = TRUE,
  allow_par = TRUE,
  parallel_over = "everything",
  save_workflow = TRUE,
  verbose_elim = TRUE
)

weight_results = weight_models %>%
  workflow_map(
    seed = 510,
    fn = "tune_race_anova",
    resamples = anrr_folds,
    grid = tuning_grid_size,
    control = race_ctrl,
    verbose = TRUE,
    metrics = tune_metrics
  )

no_weight_results = no_weight_models %>%
  workflow_map(
    seed = 510,
    fn = "tune_race_anova",
    resamples = anrr_folds,
    grid = tuning_grid_size,
    control = race_ctrl,
    verbose = TRUE,
    metrics = tune_metrics
  )

grid_results = bind_rows(weight_results, no_weight_results)
```

```{r}
write_rds(grid_results, "grid_results.rds")
```

### Ensemble model

We create the ensemble model by stacking the predicted probabilities from the individual models and then using those predicted probabilities to model non-response.

```{r ensemble_stack}
#| echo: true

ensemble_stack = stacks() %>% 
  add_candidates(candidates = grid_results) %>% 
  stacks:::process_data_stack()
```

To evaluate the ensemble we reassemble the cross-validation folds, using the model predictions instead of the observed data.

```{r ensemble_folds}
#| echo: true

ensemble_folds = anrr_folds %>%
  tidy() %>% 
  chop(Row) %>% 
  pivot_wider(names_from = Data, values_from = Row) %>% 
  select(Analysis, Assessment) %>% 
  pmap(
    function(Analysis, Assessment)
      list(analysis = Analysis, assessment = Assessment)
  ) %>% 
  map(make_splits, ensemble_stack) %>% 
  manual_rset(
    map2_chr(anrr_folds$id, anrr_folds$id2, paste, sep = "_")
  ) 
```

Then we fit LASSO logistic regressions with a set of candidate penalty terms

```{r ensemble_tuning}
#| echo: true
ensemble_penalty_candidates = 10^seq(-2, -0.5, length = 20)

ensemble_wflow = workflow() %>% 
  add_model(
    logistic_reg(penalty = tune(), mixture = tune()) %>% 
      set_engine("glmnet") %>% 
      set_mode("classification")
  ) %>% 
  add_recipe(
    recipe(
      non_response ~ .,
      data = ensemble_stack
    )
  )

ensemble_results = tune_grid(
  ensemble_wflow,
  resamples = ensemble_folds,
  grid = expand_grid(
    penalty = ensemble_penalty_candidates,
    mixture = 1
  ),
  metrics = tune_metrics,
  control = control_grid(
    save_pred = TRUE,
    allow_par = TRUE,
    parallel_over = "everything",
    save_workflow = TRUE
  )
) 

ensemble_set = as_workflow_set(ensemble = ensemble_results)
```

```{r}
write_rds(ensemble_set, "ensemble_set.rds")
```

## Selecting a model

We selected the logistic regression because it is the simplest model and there are no other models with statistically significant performance increases.

### Area under the receiver operator curve

While the ensemble and neural net have slightly higher AUC, the advantage over the logistic regression is not statistically significant.

```{r}
#| echo: true
selection_metric = "roc_auc"

best_ensemble_metrics = rank_results(
  ensemble_set, rank_metric = selection_metric, select_best = TRUE
) %>% 
  filter(.metric == selection_metric) %>%
  mutate(model_label = "ensemble")

best_metrics = rank_results(
  grid_results, rank_metric = selection_metric, select_best = TRUE
) %>% 
  filter(.metric == selection_metric) %>%
  mutate(model_label = str_remove(wflow_id, "basic_"))

best_metrics  %>% 
  bind_rows(best_ensemble_metrics) %>%
  mutate(rank = min_rank(-1 * mean)) %>% 
  arrange(rank) %>% 
  ggplot(aes(x = rank, y = mean, color = model_label)) +
  geom_errorbar(
    aes(
      ymin = mean - 1.96 * std_err,
      ymax = mean + 1.96 * std_err
    )
  ) +
  geom_point() +
  scale_x_continuous(breaks = scales::breaks_width(1)) +
  labs(
    title = "Area under the receiver operator curve",
    x = "Rank",
    y = NULL,
    color = NULL
  )
```

### Predicted probability distribution

The logistic model has a similar probability distribution to the other models and shows no reason to be concerned about its performance.

```{r}
best_predictions = collect_predictions(
  grid_results, select_best = TRUE, metric = "roc_auc"
) %>% 
  mutate(model_label = str_remove(wflow_id, "basic_"))

ensemble_predictions = collect_predictions(
  ensemble_set, select_best = TRUE, metric = "roc_auc"
) %>% 
  mutate(model_label = "ensemble")

best_predictions %>% 
  bind_rows(ensemble_predictions) %>% 
  ggplot(aes(x = `.pred_Non-Response`, y = model_label, fill = model_label)) +
  geom_density_ridges() +
  scale_y_discrete(position = "right") +
  facet_wrap(vars(non_response), ncol = 1) +
  guides(fill = "none") +
  labs(
    title = "Distribution of predicted probability of non-response",
    subtitle = "By actual response status",
    x = "Predicted non-response probability",
    y = NULL
  )

best_predictions %>% 
  bind_rows(ensemble_predictions) %>%
  ggplot(aes(x = `.pred_Non-Response`, color = model_label)) +
  stat_ecdf() +
  labs(
    title = "Cumulative distribution of predicted probability of non-response",
    x = "Predicted non-response probability",
    y = NULL,
    color = NULL
  )
```

### Calibration plot

In the calibration plot, we split the predicted probabilities into 5 bins: 0-10%, 10-20%, 20-30%, 30-40%, & 40%+. Then we take the average predicted probability of non-response in each bin and plot it against the actual non-response rate in each bin. The closer the model is to the 45 degree dashed line the better it performs.

The logistic regression closely matches the 45 degree line for the first 4 bins. Like the other models, its predictions for observations with the greatest probability of non-response are somewhat overstated.

```{r}
best_predictions %>% 
  bind_rows(ensemble_predictions) %>% 
  mutate(
    pr_bin = cut(
      `.pred_Non-Response`,
      breaks = c(seq(0, 0.4, 0.1), 1),
      labels = seq(0, 0.4, 0.1)
    ) 
  ) %>% 
  group_by(model_label, pr_bin) %>% 
  summarise(
    n = n(),
    avg_pred_prob = mean(`.pred_Non-Response`),
    pct_nonresponse = mean(non_response == "Non-Response")
  ) %>% 
  ggplot(
    aes(
      x = avg_pred_prob, y = pct_nonresponse,
      group = model_label, color = model_label
    )
  ) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  scale_y_continuous(limits = c(0, 0.5)) +
  labs(
    title = "Prediction calibration plot",
    subtitle = "Average predicted vs. actual non-response rate",
    x = "Bin avg. predicted non-response rate",
    y = "Bin avg. actual non-response rate",
    color = NULL,
    caption = "Dashed line represents perfect alignment between predicted and actual.\nThe 5 bins of predicted probability are: 0-10%, 10-20%, 20-30%, 30-40%, & 40%+."
  )
```

## Fitting the full model

```{r}
chosen_model = "basic_logit_glm"

best_chosen_model = grid_results %>% 
  extract_workflow_set_result(chosen_model) %>% 
  select_best(metric = selection_metric)

final_model = grid_results %>% 
  extract_workflow(chosen_model) %>% 
  finalize_workflow(best_chosen_model) %>% 
  last_fit(split = anrr_split)

fitted_final_model = extract_workflow(final_model)
```

### Model summary
```{r}
term_names = hardhat::get_levels(anrr_weights) %>%
  unlist()
names(term_names) = make.names(term_names)

tidy(fitted_final_model, exponentiate = TRUE) %>% 
  filter(term != "(Intercept)") %>% 
  separate(term, c("factor", "term"), sep = "_(?!.*_)") %>% 
  left_join(as_tibble(reference_levels, rownames = "factor"), by = "factor") %>% 
  mutate(
    factor = str_replace_all(factor, "_", " "),
    factor = str_to_title(factor),
    factor = glue("**{factor}** *(Ref. level: {value})*"),
    term = str_replace_all(term, term_names)
  ) %>% 
  select(-value) %>% 
  gt(groupname_col = "factor", process_md = TRUE) %>% 
  cols_label(
    term = "",
    estimate = "Odds Ratio",
    std.error = "Std. Error",
    statistic = "Z-Statistic",
    p.value = "p-value"
  ) %>% 
  fmt_number(c(estimate, std.error, statistic), decimals = 2) %>% 
  fmt(p.value, fns = gtsummary::style_pvalue) %>% 
  tab_style(
    cell_text(weight = "bold"),
    cells_column_labels()
  ) %>% 
  tab_style(
    cell_fill(color = "#00000000"),
    cells_row_groups()
  )
```

### Final ROC curve
```{r}
final_roc_auc = collect_metrics(final_model) %>%
  filter(.metric == "roc_auc") %>%
  pull(.estimate) %>% 
  round(digits = 3)

final_model %>% 
  collect_predictions() %>% 
  roc_curve(non_response, `.pred_Non-Response`, event_level = "second") %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line() +
  geom_abline(linetype = "dashed") +
  labs(
    title = "Final model ROC curve",
    subtitle = glue("AUC: {final_roc_auc}")
  )
```

### Final calibration plot
```{r}
final_model %>% 
  collect_predictions() %>% 
  mutate(
    pr_bin = cut(
      `.pred_Non-Response`,
      breaks = c(seq(0, 0.4, 0.1), 1),
      labels = seq(0, 0.4, 0.1)
    ) 
  ) %>% 
  group_by(pr_bin) %>% 
  summarise(
    n = n(),
    avg_pred_prob = mean(`.pred_Non-Response`),
    pct_nonresponse = mean(non_response == "Non-Response"),
  ) %>% 
  ggplot(aes(x = avg_pred_prob, y = pct_nonresponse)) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  scale_y_continuous(limits = c(0, 0.6)) +
  labs(
    title = "Prediction calibration plot",
    subtitle = "Average predicted vs. actual non-response rate",
    x = "Bin avg. predicted non-response rate",
    y = "Bin avg. actual non-response rate",
    caption = "Dashed line represents perfect alignment between predicted and actual.\nThe 5 bins of predicted probability are: 0-10%, 10-20%, 20-30%, 30-40%, & 40%+."
  ) +
  theme(panel.grid.minor = element_line(color = "grey90"))
```

```{r}
butchered_model = butcher(fitted_final_model)

butchered_model$pre$case_weights = NULL
butchered_model$pre$mold$blueprint$recipe$tr_info = NULL
butchered_model$pre$mold$extras$roles$case_weights = NULL
butchered_model$fit$fit$fit$residuals = NULL
butchered_model$fit$fit$fit$formula = NULL
butchered_model$fit$fit$fit$linear.predictors = NULL
butchered_model$fit$fit$fit$qr$qr = NULL
butchered_model$fit$fit$fit$model = NULL
butchered_model$fit$fit$fit$effects = NULL
butchered_model$fit$fit$fit$weights = NULL
butchered_model$fit$fit$fit$prior.weights = NULL

write_rds(
  list(
    model = butchered_model,
    prepare_survey_data = prepare_survey_data,
    prepare_model_data = prepare_model_data
  ),
  "non_response_model.rds"
)
```
